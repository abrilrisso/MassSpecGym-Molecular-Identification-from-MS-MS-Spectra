{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f7fef9",
   "metadata": {},
   "source": [
    "# MassSpecGym De Novo Task - Abril Risso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721b8cd",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fccd20",
   "metadata": {},
   "source": [
    "#### 1.1 Problem Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b36a46",
   "metadata": {},
   "source": [
    "In retrieval, the model only needed to rank a list of pre-defined candidates. Now, in the De Novo setting, the model must construct the molecule from scratch based on the input mass spectrum.\n",
    "\n",
    "This is a Seq2Seq problem:\n",
    "- **Input:** A mass spectrum (set of peaks $m/z, I$).\n",
    "- **Output:** A SMILES string (text sequence representing the molecular structure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960a29d",
   "metadata": {},
   "source": [
    "#### 1.2 Molecule Generation - De Novo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a310f03",
   "metadata": {},
   "source": [
    "In the retrieval task, the answer was in the database, but a generative model must learn the \"grammar\" of chemistry. So it needs to:\n",
    "- Underestand the spectrum (map spectral peaks to chemical substructures)\n",
    "- Generate valid SMILES to ensure that the output syntax is correct\n",
    "- Respect chemical rules (the molecule generated must match the precursor mass and must construct a chemically valid graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696ee6e",
   "metadata": {},
   "source": [
    "#### 1.3 Proposed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71ca18",
   "metadata": {},
   "source": [
    "In this notebook, it is implemented a Spectral Encoder-Decoder Transformer. The Encoder processes the spectrum (similar to the retrieval model) to create a rich representation of the peaks, and the Decoder generates the SMILES string token by token, attending to the spectral features to decide which atom comes next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045cbb7",
   "metadata": {},
   "source": [
    "## 2. Spectral Seq2Seq Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773ff84",
   "metadata": {},
   "source": [
    "Unlike standard regression or classification, our goal for the De Novo task is like translation. We are effectively translating the biological language of mass spectrometry (fragmentation patterns) into the chemical language of SMILES strings.\n",
    "\n",
    "A molecule is not a single fixed-size label, instead, it is represented as a sequence of tokens with variable length and strict syntax constraints (parentheses, ring closures,...). So, predicting the whole molecule at once would require fixing a maximum length and predicting tokens for all positions in parallel, predicting where the sequence ends and enforcing global consistency. In practice, these constraints are hard to enforce with a single-shot predictor and small local mistakes can make the entire SMILES invalid.\n",
    "\n",
    "To address this, it was decided to implement an Autoregressive Decoder. This architecture generates the molecule one token at a time, conditioning the prediction of the next character on the spectral input and also on the previously generated tokens, ensuring syntactical validity and chemical consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb7e65",
   "metadata": {},
   "source": [
    "#### 2.1 The Encoder (Spectrum Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc275a",
   "metadata": {},
   "source": [
    "The role of the encoder is to map the sparse list of spectral peaks into a rich representation that the decoder can understand. The base of this encoder is the same as the Spectral Transformer used in the Retrieval task. It treats the spectrum as a set of peaks and has the same improvements detailed in the previous notebook:\n",
    "- **Fourier Features:** To capture high-precision mass differences (isotopes).\n",
    "- **Log-Intensity & Precursor Injection:** To handle dynamic range and provide global context about the parent mass.\n",
    "- **Self-Attention:** To learn non-local relationships between peaks.\n",
    "\n",
    "However, there is one key addition to the De Novo task, the Rank Embeddings.\n",
    "\n",
    "In the Retrieval task, the encoder output was pooled into a single global vector (using Attention Pooling) to match a fingerprint. In the De Novo task, the output is not pooled. The decoder needs to attend to individual peaks to decide which atom to generate next. To help the model understand that not all peaks are equally important, a learnable embedding based on the **intensity rank** of the peak is added.\n",
    "\n",
    "This embedding is not a fixed static number, instead, it is a parameter that the model updates and optimizes during training. Initially, the model treats all ranks randomly. However, as it trains and learns from its errors, it modifies these embeddings to discover the optimal hierarchy on its own.\n",
    "\n",
    "Unlike Natural Language Processing (NLP), where positional embeddings follow the linear order of words because position defines meaning (syntax), applying a similar logic here (sorting peaks by mass) is suboptimal, since the precise $m/z$ value is already encoded by Fourier Features, with mass-based ordering we would be adding redundant information.\n",
    "\n",
    "Instead, we use **Intensity Ranking**. In mass spectrometry, intensity tells us which fragments are the most abundant and relevant. It provides the decoder with clear guidance, encouraging the model to attend to the strongest evidence first (to find the main molecular skeleton) before considering weaker signals or noise to fill in the details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db2d90",
   "metadata": {},
   "source": [
    "#### 2.2 The Decoder (Molecule Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a2baa",
   "metadata": {},
   "source": [
    "The Decoder is responsible for generating the molecular structure (SMILES sequence) token by token based on the encoded spectral representation. So, a standard Transformer Decoder is implemented with 3 key components:\n",
    "\n",
    "##### 2.2.1 Autoregressive Generation and Masked Self-Attention\n",
    "\n",
    "The Decoder operates autoregressively, meaning it predicts the next token $x_t$ based on the previously generated tokens $x_{<t}$. Moreover, in the training process, it is necessary to prevent the model from seeing the future. To achieve this, a **Masked Self-Attention** is applied, which masks out future positions (setting their attention weights of the future tokens to $-\\infty$). This forces the model to rely only on the **previous generated** tokens to predict the next step, ensuring it effectively learns the sequential grammar of chemistry.\n",
    "\n",
    "##### 2.2.2 Cross-Attention\n",
    "\n",
    "This mechanism acts as the bridge connecting the spectral data to the generation process. As explained in the previous section, the Encoder output is not pooled. It remains a matrix of feature vectors representing individual peaks. \n",
    "\n",
    "Through Cross-Attention, the Decoder uses the current partial SMILES sequence as the **query** to inspect the spectral peaks. At each generation step, the mechanism calculates attention weights over the encoded spectral peaks (the output of the Encoder). This allows the Decoder to selectively attend to the most relevant spectral peaks (prioritized by the Rank Embeddings) to determine which atom to generate next.\n",
    "\n",
    "##### 2.2.3 Positional Encoding\n",
    "\n",
    "Finally, unlike the Encoder where Intensity Ranking was used, the Decoder processes a text sequence (SMILES) where linear order determines syntax and meaning. Therefore, the standard sinusoidal Positional Encodings is used to provide the model information about the position of each token in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7d5d3",
   "metadata": {},
   "source": [
    "#### 2.3 Training Strategy: Teacher Forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f6da7",
   "metadata": {},
   "source": [
    "Training a Seq2Seq model can produce **error propagation**. If the model sees its own generated tokens from the previous step as the input for the next step, any mistake in the sequence can confuse the model for all subsequent steps. This makes convergence slow and unstable, as the model spends most of the time trying to recover from its own errors rather than learning the correct structure.\n",
    "\n",
    "To solve this, the **Teacher Forcing** strategy is implemented. Regardless of what the model predicts, the ground truth is always given as the input for the next step to the Decoder.\n",
    "\n",
    "This ensures that at every step, the model is trying to predict the next atom given a perfect history, which stabilizes gradients and speeds up the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e801a3",
   "metadata": {},
   "source": [
    "#### 2.4 Inference Strategy: Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d9b7ae",
   "metadata": {},
   "source": [
    "During training, Teacher Forcing masks the model's mistakes. However, during inference, the ground truth is unknown, so the model must rely on its own predictions to generate the next token.\n",
    "\n",
    "Therefore, a simple Greedy Search (selecting only the single highest-probability token at each step) is risky. If the model makes a single mistake early in the sequence, it leads to an error propagation from which it cannot recover, resulting in invalid or incorrect structures.\n",
    "\n",
    "To mitigate this, **Beam Search** is implemented. So, instead of relying on a single output path, the algorithm explores multiple potential sequences in parallel. It maintains a set of $k$ candidate sequences with the highest cumulative probabilities (where $k$ is the beam width) at each decoding step.\n",
    "\n",
    "The process is iterative as the model extends all the currently active candidates by predicting the next possible atoms for each one. Then, it ranks the resulting sequences by their cumulative probability and selects the top $k$ candidates to continue with the generation.\n",
    "\n",
    "It is a safer technique than the Greedy Search, as it mitigates the risk of getting trapped in a local optimum. Beam search increases the probability that the model finds the globally optimal structure, even if some intermediate steps were not the absolute highest probability choices in isolation (although it depends on the parameter k)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d0009",
   "metadata": {},
   "source": [
    "#### 2.5 Optimization and Regularization Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13014e",
   "metadata": {},
   "source": [
    "To ensure stability and prevent overfitting three key strategies were implemented:\n",
    "\n",
    "The model is trained to minimize the **Cross-Entropy Loss** between the predicted probability distribution and the ground truth SMILES tokens. However, standard Cross-Entropy can lead to overconfidence, where the model assigns extremely high probabilities to its predictions, making it weak to noise. To mitigate this, Label Smoothing ($\\epsilon=0.1$) is applied. Instead of targeting a hard probability of 1.0 for the correct token, the model targets $1.0 - \\epsilon$, distributing the remaining probability mass among other tokens. This encourages the network to learn more robust representations and creates a more clustered feature space.\n",
    "\n",
    "To prevent the model from memorizing specific noisy peaks in the training data, a small percentage of peaks is randomly masked (removed) from the input spectrum during each training step. This forces the network to learn the global fragmentation pattern of the molecule rather than relying on isolated signals.\n",
    "\n",
    "Transformers are sensitive to the learning rate during the initial phases of training. Therefore, the AdamW optimizer is implemented in combination with a Warmup and Cosine Decay. This strategy ensures the training process begins smoothly and prevents the model from diverging in the early stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d5baf",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc670e",
   "metadata": {},
   "source": [
    "#### 3.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aeff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Fourier Feature mapping for high-frequency coordinate transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.num_freqs = output_dim // 2\n",
    "        self.register_buffer('B', torch.randn(self.num_freqs) * sigma)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Normalization to prevent extreme values during projection\n",
    "        projected = 2 * math.pi * torch.clamp(x / 1000.0, 0, 2) * self.B\n",
    "        return torch.cat([torch.sin(projected), torch.cos(projected)], dim=-1)\n",
    "\n",
    "\n",
    "class PeakEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Encoder for Mass Spectrometry peaks.\n",
    "    \n",
    "    Key Components:\n",
    "    - Fourier Features for m/z representation.\n",
    "    - Linear projection for log-intensity.\n",
    "    - Learnable Rank Embeddings to prioritize high-intensity peaks.\n",
    "    - Precursor m/z injection for conditioning.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, nhead=4, num_layers=2, dropout=0.1, max_peaks=1000, use_rank_emb=True):\n",
    "        super().__init__()\n",
    "        # Features for m/z and linear projection for log-intensity\n",
    "        self.mz_enc = FourierFeatures(d_model // 2, sigma=10.0)\n",
    "        self.int_enc = nn.Linear(1, d_model - (d_model // 2))\n",
    "        \n",
    "        # Positional/Rank Embedding\n",
    "        self.use_rank_emb = use_rank_emb\n",
    "        self.max_peaks = max_peaks\n",
    "        if self.use_rank_emb:\n",
    "            self.rank_emb = nn.Embedding(max_peaks, d_model)\n",
    "        \n",
    "        # Projection for the precursor m/z\n",
    "        self.precursor_proj = nn.Linear(1, d_model)\n",
    "        \n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, \n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "    \n",
    "    def forward(self, spec, precursor_mz, src_key_padding_mask=None):\n",
    "        mz = spec[:, :, 0:1]\n",
    "        intensity = spec[:, :, 1:2]\n",
    "        B_size, N_peaks, _ = spec.shape\n",
    "        \n",
    "        mz_emb = self.mz_enc(mz)\n",
    "        int_emb = self.int_enc(torch.log1p(torch.clamp(intensity, 0, 1e6))) # Use log1p and clamp to avoid log(0) or extremely large values\n",
    "        \n",
    "        x = torch.cat([mz_emb, int_emb], dim=-1)\n",
    "        \n",
    "        # Add Rank Embeddings\n",
    "        if self.use_rank_emb:\n",
    "            positions = torch.arange(N_peaks, device=spec.device).unsqueeze(0).expand(B_size, -1)\n",
    "            positions = positions.clamp(max=self.max_peaks - 1)\n",
    "            x = x + self.rank_emb(positions)\n",
    "        \n",
    "        # Precursor Injection\n",
    "        if precursor_mz is not None:\n",
    "            if precursor_mz.dim() == 1:\n",
    "                precursor_mz = precursor_mz.unsqueeze(-1)\n",
    "            prec_norm = torch.clamp(precursor_mz.float() / 1000.0, 0, 2)\n",
    "            prec_emb = self.precursor_proj(prec_norm).unsqueeze(1)\n",
    "            x = x + prec_emb\n",
    "        \n",
    "        x = self.input_norm(x)\n",
    "        \n",
    "        memory = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d2830",
   "metadata": {},
   "source": [
    "#### 3.2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional embeddings for the SMILES sequence.\n",
    "    Allows the model to understand the order of atoms in the string.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.encoding = nn.Embedding(max_len, d_model)\n",
    "        self.register_buffer(\"positions\", torch.arange(max_len))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        clamped_positions = self.positions[:seq_len].clamp(max=self.encoding.num_embeddings - 1)\n",
    "        return self.encoding(clamped_positions.unsqueeze(0))\n",
    "\n",
    "\n",
    "class AutoregressiveDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Transformer Decoder.\n",
    "    Predicts the next SMILES token based on the Encoder memory and previous tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dropout=0.1, max_len=200):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = LearnedPositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model, nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoder(tgt)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.transformer_decoder(\n",
    "            tgt=x,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        return self.output_proj(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3ef0b",
   "metadata": {},
   "source": [
    "#### 3.3 Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDeNovoTransformer(DeNovoMassSpecGymModel):\n",
    "    \"\"\"\n",
    "    Main PyTorch Lightning Module for De Novo Molecular Generation.\n",
    "    \n",
    "    Integrates:\n",
    "    - PeakEncoder (Spectral processing)\n",
    "    - AutoregressiveDecoder (SMILES generation)\n",
    "    - Training logic (Teacher Forcing, Loss calculation)\n",
    "    - Inference logic (Beam Search)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        smiles_tokenizer: SpecialTokensBaseTokenizer,\n",
    "        dropout: float = 0.1,\n",
    "        max_smiles_len: int = 150,\n",
    "        lr: float = 5e-4,\n",
    "        peak_dropout_p: float = 0.1,\n",
    "        label_smoothing: float = 0.1,\n",
    "        warmup_ratio: float = 0.1,\n",
    "        beam_size: int = 5,\n",
    "        length_penalty_alpha: float = 0.8,\n",
    "        max_peaks: int = 1000,\n",
    "        top_ks: list = None,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if top_ks is None:\n",
    "            top_ks = [1, 5]\n",
    "        super().__init__(top_ks=top_ks, *args, **kwargs)\n",
    "        self.save_hyperparameters(ignore=['smiles_tokenizer'])\n",
    "        \n",
    "        self.smiles_tokenizer = smiles_tokenizer\n",
    "        self.vocab_size = smiles_tokenizer.get_vocab_size()\n",
    "        self.pad_id = smiles_tokenizer.token_to_id(PAD_TOKEN)\n",
    "        self.sos_id = smiles_tokenizer.token_to_id(SOS_TOKEN)\n",
    "        self.eos_id = smiles_tokenizer.token_to_id(EOS_TOKEN)\n",
    "        self.max_len = max_smiles_len\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.peak_dropout_p = peak_dropout_p\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.beam_size = beam_size\n",
    "        self.length_penalty_alpha = length_penalty_alpha\n",
    "        \n",
    "        self.max_k_eval = max(top_ks) if top_ks else 1\n",
    "        \n",
    "        self.encoder = PeakEncoder(\n",
    "            d_model, nhead, num_encoder_layers, dropout,\n",
    "            max_peaks=max_peaks, use_rank_emb=True\n",
    "        )\n",
    "        self.decoder = AutoregressiveDecoder(\n",
    "            self.vocab_size, d_model, nhead, num_decoder_layers,\n",
    "            dropout, max_smiles_len\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            ignore_index=self.pad_id,\n",
    "            label_smoothing=label_smoothing\n",
    "        )\n",
    "    \n",
    "    def generate_src_padding_mask(self, spec):\n",
    "        return spec.abs().sum(dim=-1) == 0\n",
    "    \n",
    "    def generate_causal_mask(self, sz, device):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf'), device=device), diagonal=1)\n",
    "    \n",
    "    def _augment_spectrum(self, spec):\n",
    "        \"\"\"Applies Peak Dropout only during training to improve robustness.\"\"\"\n",
    "        if self.training and self.peak_dropout_p > 0:\n",
    "            is_content = (spec.abs().sum(dim=-1) > 0).float()\n",
    "            dropout_mask = torch.bernoulli(torch.full_like(is_content, self.peak_dropout_p))\n",
    "            final_mask = dropout_mask * is_content\n",
    "            spec = spec * (1 - final_mask.unsqueeze(-1))\n",
    "        return spec\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        spec = batch[\"spec\"]\n",
    "        precursor_mz = batch.get(\"precursor_mz\", None)\n",
    "        mols = batch[\"mol\"]\n",
    "        \n",
    "        spec = self._augment_spectrum(spec)\n",
    "        \n",
    "        encoded_mols = self.smiles_tokenizer.encode_batch(mols)\n",
    "        tgt_ids = torch.tensor([e.ids for e in encoded_mols], device=self.device)\n",
    "        \n",
    "        tgt_in = tgt_ids[:, :-1]\n",
    "        tgt_out = tgt_ids[:, 1:]\n",
    "        \n",
    "        src_mask = self.generate_src_padding_mask(spec)\n",
    "        tgt_pad_mask = (tgt_in == self.pad_id)\n",
    "        tgt_causal_mask = self.generate_causal_mask(tgt_in.size(1), self.device)\n",
    "        \n",
    "        memory = self.encoder(spec, precursor_mz, src_key_padding_mask=src_mask)\n",
    "        logits = self.decoder(\n",
    "            tgt_in, memory,\n",
    "            tgt_mask=tgt_causal_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        \n",
    "        return logits, tgt_out\n",
    "    \n",
    "    def step(self, batch, stage: Stage = Stage.NONE):\n",
    "        logits, tgt_out = self.forward(batch)\n",
    "        \n",
    "        loss = self.criterion(logits.reshape(-1, self.vocab_size), tgt_out.reshape(-1))\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN detected in loss at stage {stage}\")\n",
    "            loss = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "        \n",
    "        mols_pred = None\n",
    "        if stage not in self.log_only_loss_at_stages:\n",
    "            mols_pred = self.decode_smiles(batch)\n",
    "        \n",
    "        return dict(loss=loss, mols_pred=mols_pred)\n",
    "    \n",
    "    def decode_smiles(self, batch):\n",
    "        \"\"\"\n",
    "        Executes Beam Search decoding with Early Stopping.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Maintains top-k sequences (beams) at each step.\n",
    "        2. Prunes low-probability paths.\n",
    "        3. Stops when enough hypotheses (k) are finished or max length is reached.\n",
    "        \"\"\"\n",
    "        spec = batch[\"spec\"]\n",
    "        precursor_mz = batch.get(\"precursor_mz\", None)\n",
    "        batch_size = spec.size(0)\n",
    "        beam_size = self.beam_size\n",
    "        device = self.device\n",
    "        required_k = self.max_k_eval\n",
    "        \n",
    "        src_mask = self.generate_src_padding_mask(spec)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            # Encoder forward pass\n",
    "            memory = self.encoder(spec, precursor_mz, src_key_padding_mask=src_mask)\n",
    "            memory = memory.repeat_interleave(beam_size, dim=0)\n",
    "            src_mask = src_mask.repeat_interleave(beam_size, dim=0)\n",
    "            \n",
    "            # Initialize Beams\n",
    "            ys = torch.full((batch_size * beam_size, 1), self.sos_id, dtype=torch.long, device=device)\n",
    "            beam_scores = torch.zeros((batch_size, beam_size), device=device)\n",
    "            beam_scores[:, 1:] = float('-1e9') # Only the first beam starts with 0 score\n",
    "            beam_scores = beam_scores.view(-1)\n",
    "            \n",
    "            finished_beams = [[] for _ in range(batch_size)]\n",
    "            \n",
    "            for step in range(self.max_len):\n",
    "                # EARLY STOPPING CHECK\n",
    "                finished_counts = torch.tensor([len(fb) for fb in finished_beams], device=device)\n",
    "                has_enough_hyps = (finished_counts >= required_k)\n",
    "                \n",
    "                active_scores_view = beam_scores.view(batch_size, beam_size)\n",
    "                has_active_beams = (active_scores_view > -1e8).any(dim=1)\n",
    "                \n",
    "                needs_work = (~has_enough_hyps) & has_active_beams\n",
    "                if not needs_work.any():\n",
    "                    break\n",
    "                \n",
    "                # Decoder Step\n",
    "                tgt_mask = self.generate_causal_mask(ys.size(1), device)\n",
    "                out = self.decoder(ys, memory, tgt_mask=tgt_mask, memory_key_padding_mask=src_mask)\n",
    "                logits = out[:, -1, :]\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                next_scores = log_probs + beam_scores.unsqueeze(-1)\n",
    "                next_scores = next_scores.view(batch_size, -1)\n",
    "                \n",
    "                topk_scores, topk_indices = next_scores.topk(beam_size, dim=1)\n",
    "                beam_indices = torch.div(topk_indices, self.vocab_size, rounding_mode='floor')\n",
    "                token_indices = topk_indices % self.vocab_size\n",
    "                \n",
    "                batch_offset = torch.arange(batch_size, device=device).unsqueeze(1) * beam_size\n",
    "                global_beam_indices = batch_offset + beam_indices\n",
    "                \n",
    "                prev_ys = ys[global_beam_indices.view(-1)]\n",
    "                new_tokens = token_indices.view(-1, 1)\n",
    "                ys = torch.cat([prev_ys, new_tokens], dim=1)\n",
    "                beam_scores = topk_scores.view(-1)\n",
    "                \n",
    "                # EOS Check\n",
    "                current_tokens = token_indices.view(-1)\n",
    "                is_eos = (current_tokens == self.eos_id)\n",
    "                \n",
    "                if is_eos.any():\n",
    "                    eos_indices = torch.nonzero(is_eos, as_tuple=True)[0]\n",
    "                    for idx in eos_indices:\n",
    "                        batch_idx = idx.item() // beam_size\n",
    "                        score = beam_scores[idx].item()\n",
    "                        seq = ys[idx].tolist()\n",
    "                        # Length Penalty application\n",
    "                        lp = ((5 + len(seq)) / 6) ** self.length_penalty_alpha\n",
    "                        final_score = score / lp\n",
    "                        finished_beams[batch_idx].append((final_score, seq))\n",
    "                        beam_scores[idx] = float('-1e9') # Invalidate this beam\n",
    "            \n",
    "            decoded_smiles_batch = []\n",
    "            for i in range(batch_size):\n",
    "                # Collect beams that didn't finish\n",
    "                for j in range(beam_size):\n",
    "                    idx = i * beam_size + j\n",
    "                    if beam_scores[idx] > float('-1e8'):\n",
    "                        seq = ys[idx].tolist()\n",
    "                        lp = ((5 + len(seq)) / 6) ** self.length_penalty_alpha\n",
    "                        score = beam_scores[idx].item() / lp\n",
    "                        finished_beams[i].append((score, seq))\n",
    "                \n",
    "                finished_beams[i].sort(key=lambda x: x[0], reverse=True)\n",
    "                max_k = self.max_k_eval\n",
    "                top_hyps = finished_beams[i][:max_k]\n",
    "                \n",
    "                batch_preds = []\n",
    "                for _, seq in top_hyps:\n",
    "                    try:\n",
    "                        s = self.smiles_tokenizer.decode(seq, skip_special_tokens=True)\n",
    "                    except:\n",
    "                        s = \"\"\n",
    "                    batch_preds.append(s)\n",
    "                \n",
    "                # Fill with empty string if no hypotheses found or fewer than max_k\n",
    "                if len(batch_preds) == 0:\n",
    "                    batch_preds = [\"\"] * max_k\n",
    "                elif len(batch_preds) < max_k:\n",
    "                    # Fill with the last valid prediction or empty string\n",
    "                    fill_value = batch_preds[-1] if batch_preds else \"\"\n",
    "                    batch_preds += [fill_value] * (max_k - len(batch_preds))\n",
    "                \n",
    "                decoded_smiles_batch.append(batch_preds)\n",
    "            \n",
    "            return decoded_smiles_batch\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        \n",
    "        if hasattr(self, 'trainer') and self.trainer is not None:\n",
    "            total_steps = self.trainer.estimated_stepping_batches\n",
    "        else:\n",
    "            total_steps = 1000\n",
    "        \n",
    "        warmup_steps = int(total_steps * self.warmup_ratio)\n",
    "        \n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < warmup_steps:\n",
    "                return float(current_step) / float(max(1, warmup_steps))\n",
    "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "        \n",
    "        scheduler = {\n",
    "            'scheduler': LambdaLR(optimizer, lr_lambda),\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d48a1",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b3c28",
   "metadata": {},
   "source": [
    "The following table summarizes the performance of the De Novo model on the test set.\n",
    "\n",
    "| Metric | Value | Description |\n",
    "| :--- | :---: | :--- |\n",
    "| **Test Loss** | 1.653 | Cross-Entropy loss between predicted and actual tokens. |\n",
    "| **Top-1 Accuracy** | 0.00% | Percentage of exact structure matches (perfect predictions). |\n",
    "| **Top-1 Tanimoto** | 0.108 | Structural similarity of the best prediction to the ground truth (0-1). |\n",
    "| **Top-1 MCES Dist** | 37.93 | Max Common Edge Subgraph Distance (lower is better). |\n",
    "| **Top-5 Accuracy** | 0.00% | Percentage where the correct structure appears in the top 5 beams. |\n",
    "| **Top-5 Tanimoto** | 0.130 | Structural similarity of the best candidate among the top 5. |\n",
    "| **Top-5 MCES Dist** | 30.87 | MCES Distance for the best candidate among the top 5. |\n",
    "\n",
    "The results highlight the significant challenge of the De Novo generation task compared to spectral retrieval. Unlike retrieval, where the answer is selected from a finite dataset, the generative model must construct the molecule atom-by-atom, exploring an infinite chemical space.\n",
    "\n",
    "While a **Top-1 Accuracy of 0.00%** might initially seem low, it is consistent with baseline performance for this specific task and dataset. As other Transformer-based architectures that are presented in the repository of Massspecgym (such as the SMILES Transformer and SELFIES Transformer) also report 0.0% Top-1 Accuracy. This confirms that exact structure reconstruction is a really difficult objective.\n",
    "\n",
    "Despite the lack of exact matches, the model demonstrates learning capability. It achieves a Top-1 Tanimoto Similarity of 0.108. It outperforms the baseline SMILES Transformer (0.03) and acts competitively with the SELFIES Transformer (0.08) reported in the benchmark. This suggests that while the model struggles to pinpoint the exact molecule, it is successfully identifying structural relevant molecular fragments and partial structures compatible with the input spectrum, performing better than random generation or basic transformer baselines.\n",
    "\n",
    "Furthermore, the utility of the inference strategy is evident when comparing Top-1 and Top-5 metrics. As the Tanimoto Similarity improves from 0.108 to 0.130 and MCES Distance improves (decreases) from 37.93 to 30.87. This confirms that the Greedy strategy leads often to a local optimum. By maintaining multiple beams, the model is able to propose alternative structures that are structurally closer to the ground truth than the single best probability prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf026b",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c38bc5",
   "metadata": {},
   "source": [
    "This project explored the challenging transition from spectral retrieval to De Novo Molecular Generation, implementing a Transformer-based Encoder-Decoder architecture designed to translate Mass Spectrometry data directly into chemical structures (SMILES).\n",
    "\n",
    "The contrast between the Retrieval task (previous notebook) and this Generative task is significant. While retrieval relies on matching fingerprints, de novo generation requires constructing a molecule atom-by-atom from an infinite chemical space. The 0.00% Top-1 Accuracy reflects the extreme difficulty of this problem when relying only on supervised learning.\n",
    "\n",
    "However, although the model did not achieve exact structure reconstruction, it reached a Top-1 Tanimoto similarity score (0.108) superior to standard baselines. This result validates the effectiveness of the proposed Rank Embeddings and Beam Search strategies, showing that the model successfully identifies relevant molecular structures even without large-scale pre-training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
